{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Project Summary: \n",
    "\n",
    "In this OpenStreetMap project with SQL, I use data wrangling techniques such as assessing the quality of the data for validity, accuracy and completeness to clean OpenStreetMap data. Then I convert the dataset from XML to CSV format, import the cleaned .csv files into database, conduct SQL queries to provide a statistical overview of the dataset. Finally, I give some additional suggestions for improving and analyzing the data.\n",
    "Map Area: Stokholm\n",
    "Split osm file into a smaller sample (SAMPLE_FILE). The original file (Stockholm) is 2GB. Challenges: activating python2 via source activate py2 to be able to run the following code(the code is written for py2). I stared with k=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET  # we can use cElementTree or lxml if too slow\n",
    "import pprint\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import cerberus\n",
    "import schema\n",
    "import sqlite3\n",
    "\n",
    "OSM_FILE = \"stockholm_sweden.osm\"  \n",
    "SAMPLE_FILE = \"sample.osm\"\n",
    "\n",
    "k = 10 # Parameter: take every k-th top level element! I started with k=10\n",
    "\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\n",
    "\n",
    "    Reference:\n",
    "    http://stackoverflow.com/questions/3095434/inserting-newlines-in-xml-file-generated-via-xml-etree-elementtree-in-python\n",
    "    \"\"\"\n",
    "    context = iter(ET.iterparse(osm_file, events=('start', 'end')))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "with open(SAMPLE_FILE, 'wb') as output:\n",
    "    output.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "    output.write('<osm>\\n  ')\n",
    "\n",
    "    # Write every kth top level element\n",
    "    for i, element in enumerate(get_element(OSM_FILE)):\n",
    "        if i % k == 0:\n",
    "            output.write(ET.tostring(element, encoding='utf-8'))\n",
    "\n",
    "    output.write('</osm>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse data-set and identify different tags, using iterative parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'member': 19825,\n",
      " 'nd': 745157,\n",
      " 'node': 610544,\n",
      " 'osm': 1,\n",
      " 'relation': 1012,\n",
      " 'tag': 216022,\n",
      " 'way': 69782}\n"
     ]
    }
   ],
   "source": [
    "def count_all_tags(samplefile):\n",
    "        all_tags=ET.iterparse(samplefile)\n",
    "        nodes= defaultdict(int)\n",
    "        for each_node in all_tags:\n",
    "            nodes[each_node[1].tag] +=1\n",
    "        return dict(nodes)           \n",
    "    \n",
    "def different_tags():\n",
    "    \n",
    "    tags = count_all_tags(SAMPLE_FILE)\n",
    "    pprint.pprint(tags)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    different_tags()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of Unique users contributed to the map in the Stockholm area:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1883\n"
     ]
    }
   ],
   "source": [
    "def process_map(filename):\n",
    "    users = set()\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        if \"uid\" in element.attrib:\n",
    "            users.add(element.get('uid'))\n",
    "\n",
    "    return users\n",
    "\n",
    "def test():\n",
    "\n",
    "    users = process_map(SAMPLE_FILE)\n",
    "    pprint.pprint(len(users))  #print number of Unique users\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auditing \n",
    "One of the usual problems in openstreetmap dataset is from the street name abbreviation which is also very culture oriented. However, only by looking at the osm file I could'nt identify an particular problem. I used a street_type auditing code to find out how well collected the data is.\n",
    "Steps i took:\n",
    "1-Building the regular expression to match the last element in the string, where usually the street type is based. \n",
    "2-Then based on the street abbreviation, create a mapping that finally needs to be cleaned up.\n",
    "\n",
    "\n",
    "I tried all sort of changes in my code, however the result looks pretty good(Swedes are really good at documentation afterall ;). It is worth noting that Swedish wording is differnet in many ways. in other words, for a street-name+street_type it usually only one world 'namestreet'; for example 'axfordstreet' as just one word. therefore, if the code recognises the last word not detected as expected will return it as an unvalid name to be updated. In this case a majority of valid and correct street names/types will be printed out. To avoid the confusion and to avoid printing 2G worth of streettypes, I wrote a code to update lower case street types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Farnaz/anaconda/envs/py2/lib/python2.7/site-packages/ipykernel/__main__.py:23: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<type 'set'>, {'torg': set(['Valla torg'])})\n",
      "defaultdict(<type 'set'>, {u'gatan': set([u'Gr\\xf6na gatan']), 'torg': set(['Valla torg'])})\n",
      "defaultdict(<type 'set'>, {u'gatan': set([u'Gr\\xf6na gatan']), 'torg': set(['Valla torg'])})\n",
      "defaultdict(<type 'set'>, {u'gatan': set([u'Gr\\xf6na gatan']), 'torg': set(['Valla torg'])})\n",
      "defaultdict(<type 'set'>, {u'gatan': set([u'Gr\\xf6na gatan']), 'torg': set(['Valla torg'])})\n",
      "defaultdict(<type 'set'>, {u'gatan': set([u'Gr\\xf6na gatan']), 'torg': set(['Valla torg'])})\n",
      "defaultdict(<type 'set'>, {u'gatan': set([u'Gr\\xf6na gatan']), 'torg': set(['Valla torg', 'Kista torg'])})\n",
      "defaultdict(<type 'set'>, {u'gatan': set([u'Gr\\xf6na gatan']), 'Gatan': set(['Tysta Gatan']), 'torg': set(['Valla torg', 'Kista torg'])})\n",
      "defaultdict(<type 'set'>, {u'gatan': set([u'Gr\\xf6na gatan']), 'Gatan': set(['Tysta Gatan']), 'torg': set(['Valla torg', 'Kista torg'])})\n",
      "defaultdict(<type 'set'>, {u'gatan': set([u'Gr\\xf6na gatan']), 'Gatan': set(['Tysta Gatan']), 'torg': set(['Valla torg', 'Kista torg']), 'Boulevard': set(['Gustav III:s Boulevard'])})\n",
      "defaultdict(<type 'set'>, {u'gatan': set([u'Gr\\xf6na gatan']), 'Gatan': set(['Tysta Gatan']), 'torg': set(['Valla torg', 'Kista torg']), 'Boulevard': set(['Gustav III:s Boulevard'])})\n",
      "defaultdict(<type 'set'>, {u'gatan': set([u'Gr\\xf6na gatan']), 'Gatan': set(['Tysta Gatan']), 'torg': set(['Valla torg', 'Kista torg']), 'Boulevard': set(['Gustav III:s Boulevard'])})\n",
      "defaultdict(<type 'set'>, {u'gatan': set([u'Gr\\xf6na gatan']), 'Gatan': set(['Tysta Gatan']), 'torg': set(['Valla torg', 'Kista torg']), 'Boulevard': set(['Gustav III:s Boulevard'])})\n",
      "defaultdict(<type 'set'>, {u'gatan': set([u'Gr\\xf6na gatan']), 'Gatan': set(['Tysta Gatan']), 'torg': set(['Valla torg', 'Kista torg']), 'Boulevard': set(['Gustav III:s Boulevard'])})\n",
      "defaultdict(<type 'set'>, {u'gatan': set([u'Gr\\xf6na gatan']), 'Gatan': set(['Tysta Gatan']), 'torg': set(['Gustaf de Lavals torg', 'Valla torg', 'Kista torg']), 'Boulevard': set(['Gustav III:s Boulevard'])})\n",
      "defaultdict(<type 'set'>, {u'gatan': set([u'Gr\\xf6na gatan']), 'Gatan': set([u'L\\xe5nga Gatan', 'Tysta Gatan']), 'torg': set(['Gustaf de Lavals torg', 'Valla torg', 'Kista torg']), 'Boulevard': set(['Gustav III:s Boulevard'])})\n",
      "defaultdict(<type 'set'>, {u'gatan': set([u'Gr\\xf6na gatan']), 'Gatan': set(['Breda Gatan', u'L\\xe5nga Gatan', 'Tysta Gatan']), 'torg': set(['Gustaf de Lavals torg', 'Valla torg', 'Kista torg']), 'Boulevard': set(['Gustav III:s Boulevard'])})\n",
      "defaultdict(<type 'set'>, {u'gatan': set([u'Gr\\xf6na gatan']), 'Gatan': set(['Breda Gatan', u'L\\xe5nga Gatan', 'Tysta Gatan']), 'torg': set(['Gustaf de Lavals torg', 'Valla torg', 'Kista torg']), 'Boulevard': set(['Gustav III:s Boulevard'])})\n",
      "defaultdict(<type 'set'>, {u'gatan': set([u'Gr\\xf6na gatan', u'\\xd6stra \\xc5gatan']), 'Gatan': set(['Breda Gatan', u'L\\xe5nga Gatan', 'Tysta Gatan']), 'torg': set(['Gustaf de Lavals torg', 'Valla torg', 'Kista torg']), 'Boulevard': set(['Gustav III:s Boulevard'])})\n",
      "defaultdict(<type 'set'>, {u'gatan': set([u'Gr\\xf6na gatan', u'\\xd6stra \\xc5gatan']), 'Gatan': set(['Breda Gatan', u'L\\xe5nga Gatan', 'Tysta Gatan']), 'torg': set(['Gustaf de Lavals torg', 'Valla torg', 'Kista torg']), 'Boulevard': set(['Gustav III:s Boulevard'])})\n",
      "defaultdict(<type 'set'>, {u'gatan': set([u'Gr\\xf6na gatan', 'Finska gatan', u'\\xd6stra \\xc5gatan']), 'Gatan': set(['Breda Gatan', u'L\\xe5nga Gatan', 'Tysta Gatan']), 'torg': set(['Gustaf de Lavals torg', 'Valla torg', 'Kista torg']), 'Boulevard': set(['Gustav III:s Boulevard'])})\n",
      "defaultdict(<type 'set'>, {u'gatan': set([u'Gr\\xf6na gatan', 'Finska gatan', u'\\xd6stra \\xc5gatan']), 'Gatan': set(['Breda Gatan', u'L\\xe5nga Gatan', 'Tysta Gatan']), 'torg': set(['Gustaf de Lavals torg', 'Valla torg', 'Kista torg']), 'Boulevard': set(['Gustav III:s Boulevard'])})\n",
      "defaultdict(<type 'set'>, {u'gatan': set([u'Gr\\xf6na gatan', 'Finska gatan', u'\\xd6stra \\xc5gatan']), 'Gatan': set(['Breda Gatan', u'L\\xe5nga Gatan', 'Tysta Gatan']), 'torg': set(['Gustaf de Lavals torg', 'Valla torg', 'Kista torg']), 'Boulevard': set(['Gustav III:s Boulevard'])})\n",
      "defaultdict(<type 'set'>, {u'gatan': set([u'Gr\\xf6na gatan', 'Finska gatan', u'\\xd6stra \\xc5gatan']), 'Gatan': set(['Breda Gatan', u'L\\xe5nga Gatan', 'Tysta Gatan']), 'torg': set(['Gustaf de Lavals torg', 'Valla torg', 'Kista torg']), 'Boulevard': set(['Gustav III:s Boulevard'])})\n",
      "defaultdict(<type 'set'>, {u'gatan': set([u'Gr\\xf6na gatan', 'Finska gatan', u'\\xd6stra \\xc5gatan', u'V\\xe4stra \\xc5gatan']), 'Gatan': set(['Breda Gatan', u'L\\xe5nga Gatan', 'Tysta Gatan']), 'torg': set(['Gustaf de Lavals torg', 'Valla torg', 'Kista torg']), 'Boulevard': set(['Gustav III:s Boulevard'])})\n",
      "defaultdict(<type 'set'>, {u'gatan': set(['Lugna gatan', u'Gr\\xf6na gatan', 'Finska gatan', u'\\xd6stra \\xc5gatan', u'V\\xe4stra \\xc5gatan']), 'Gatan': set(['Breda Gatan', u'L\\xe5nga Gatan', 'Tysta Gatan']), 'torg': set(['Gustaf de Lavals torg', 'Valla torg', 'Kista torg']), 'Boulevard': set(['Gustav III:s Boulevard'])})\n",
      "defaultdict(<type 'set'>, {u'gatan': set(['Lugna gatan', u'Gr\\xf6na gatan', 'Finska gatan', u'\\xd6stra \\xc5gatan', u'V\\xe4stra \\xc5gatan']), 'Gatan': set(['Breda Gatan', u'L\\xe5nga Gatan', 'Tysta Gatan']), 'torg': set(['Gustaf de Lavals torg', 'Valla torg', 'Kista torg']), 'Boulevard': set(['Gustav III:s Boulevard'])})\n",
      "defaultdict(<type 'set'>, {u'gatan': set(['Lugna gatan', u'Gr\\xf6na gatan', 'Finska gatan', u'\\xd6stra \\xc5gatan', u'V\\xe4stra \\xc5gatan']), 'Gatan': set(['Breda Gatan', u'L\\xe5nga Gatan', 'Tysta Gatan']), 'torg': set(['Gustaf de Lavals torg', 'Valla torg', 'Kista torg']), 'Boulevard': set(['Gustav III:s Boulevard'])})\n",
      "defaultdict(<type 'set'>, {u'gatan': set(['Lugna gatan', u'Gr\\xf6na gatan', 'Finska gatan', u'\\xd6stra \\xc5gatan', u'V\\xe4stra \\xc5gatan']), 'Gatan': set(['Breda Gatan', u'L\\xe5nga Gatan', 'Tysta Gatan']), 'torg': set(['Gustaf de Lavals torg', 'Valla torg', 'Kista torg']), 'Boulevard': set(['Gustav III:s Boulevard'])})\n",
      "defaultdict(<type 'set'>, {u'gatan': set(['Lugna gatan', u'Gr\\xf6na gatan', 'Finska gatan', u'\\xd6stra \\xc5gatan', u'V\\xe4stra \\xc5gatan']), 'Gatan': set(['Breda Gatan', u'L\\xe5nga Gatan', 'Tysta Gatan']), 'torg': set(['Gustaf de Lavals torg', 'Valla torg', 'Kista torg']), 'Boulevard': set(['Gustav III:s Boulevard'])})\n"
     ]
    }
   ],
   "source": [
    "street_types = defaultdict(set)\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "expected = [ \"Väg\", \"Gatan\", \"Alle\",\"Allé\", \"väg\",\"torg\",\"gatan\",\"alle\", \"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\",\n",
    "            \"Trail\", \"Parkway\", \"Commons\", \"Cove\", \"Alley\", \"Park\", \"Way\", \"Walk\" \"Circle\", \"Highway\",\n",
    "            \"Plaza\", \"Path\", \"Center\", \"Mission\", \"Kyrka\", \"kyrka\"]\n",
    "\n",
    "mapping = { \"väg\": \"Väg\" ,\n",
    "           \"torg\": \"Torg\",\n",
    "            \"gata\":\"Gatan\",\n",
    "            \"gatan\": \"Gatan\" ,\n",
    "            \"allé\" :\"Alle\",\n",
    "           \"boulevard\":\"Boulevard\",\n",
    "           \n",
    "            }\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "\n",
    "    pattern = street_type_re.search(street_name) #finds the pattern of last words\n",
    "    if pattern:\n",
    "        street_type = pattern.group() #returns the last word\n",
    "\n",
    "        if street_type in expected:  ## here is my own interpretation of expected (for english speaking countries i would use #\"if street_type in expected: )#\n",
    "\n",
    "            street_types[street_type].add(street_name)\n",
    "            print street_types\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    for event, elem in ET.iterparse(osmfile, events=(\"start\",)):\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return street_types\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    audit(SAMPLE_FILE)\n",
    "#    pprint.pprint(audit(SAMPLE_FILE) )\n",
    "\n",
    "#    for name, street in street_types.items():\n",
    "\n",
    "#        print(\"/nSet:\", name, \"Entries:\"),\n",
    "#        for item in street:\n",
    "\n",
    "#            print (item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lugna gatan is updated to: Lugna Gatan\n",
      "Gröna gatan is updated to: Gröna Gatan\n",
      "Finska gatan is updated to: Finska Gatan\n",
      "Östra Ågatan is updated to: Östra ÅGatan\n",
      "Västra Ågatan is updated to: Västra ÅGatan\n",
      "Breda Gatan is updated to: Breda Gatan\n",
      "Långa Gatan is updated to: Långa Gatan\n",
      "Tysta Gatan is updated to: Tysta Gatan\n",
      "Gustaf de Lavals torg is updated to: Gustaf de Lavals Torg\n",
      "Valla torg is updated to: Valla Torg\n",
      "Kista torg is updated to: Kista Torg\n",
      "Gustav III:s Boulevard is updated to: Gustav III:s Boulevard\n"
     ]
    }
   ],
   "source": [
    "def update_name(name, mapping, regex): #mapping the street type and updating them\n",
    "    nm = regex.search(name)\n",
    "    if nm:\n",
    "        st_type = nm.group()\n",
    "        if st_type in mapping:\n",
    "            name = re.sub(regex, mapping[st_type], name)\n",
    "    return name\n",
    "for street_type, ways in street_types.iteritems():\n",
    "    for name in ways:\n",
    "        better_name = update_name(name, mapping, street_type_re)\n",
    "        print name, \"is updated to:\", better_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking ‘k’ value for each tag. creating a dictionary of the different tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lower': 148584, 'lower_and_colon': 65754, 'other': 1683, 'troublemaker': 1}\n"
     ]
    }
   ],
   "source": [
    "#Regular expressions:\n",
    "lower = re.compile(r'^([a-z]|_)*$') #lower is for valid only-lowercase-letter tags.\n",
    "lower_and_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$') #lower_and_colon is for valid tags with a colon in the value. \n",
    "troublemaker = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]') #troublemaker is for tags with odd characters.\n",
    "\n",
    "\n",
    "def key_type(element, keys):\n",
    "    if element.tag == \"tag\":\n",
    "        if re.match(lower, element.attrib['k']):\n",
    "            keys[\"lower\"] += 1\n",
    "        elif re.match(lower_and_colon, element.attrib['k']):\n",
    "            keys[\"lower_and_colon\"] += 1\n",
    "        elif re.search(troublemaker, element.attrib['k']):\n",
    "            keys[\"troublemaker\"] += 1\n",
    "        else:\n",
    "            keys['other'] += 1\n",
    "    return keys\n",
    "\n",
    "\n",
    "def process_map(filename):\n",
    "    keys = {\"lower\": 0, \"lower_and_colon\": 0, \"troublemaker\": 0, \"other\": 0}\n",
    "    for  event , elem in ET.iterparse(filename):\n",
    "        keys = key_type(elem, keys)\n",
    "\n",
    "    return keys\n",
    "\n",
    "stockholm_all_tags = process_map(SAMPLE_FILE)\n",
    "pprint.pprint (stockholm_all_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auditing postal codes. The first two digit of postal codes in stockholm is 72."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def is_this_zipcode(elem):\n",
    "    return (elem.attrib['k'] == \"addr:postcode\")\n",
    "\n",
    "def audit_zipcode(invalid_zipcodes, zipcode):\n",
    "    two_digits_code = zipcode[0:2]\n",
    "    \n",
    "    if two_digits_code != 72 or not two_digits_code.isdigit():\n",
    "        invalid_zipcodes[two_digits_code].add(zipcode)\n",
    "\n",
    "\n",
    "def audit_main(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    invalid_zipcodes = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_this_zipcode(tag):\n",
    "                    audit_zipcode(invalid_zipcodes,tag.attrib['v'])\n",
    "\n",
    "    return invalid_zipcodes\n",
    "\n",
    "stokholm_zipcode = audit_main(SAMPLE_FILE)\n",
    "#pprint.pprint(dict(stokholm_zipcode))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems Encountered: Inconsistent postal codes! \n",
    "Important note: Although I indicated invalid zipcode in a broad set, many of the above zipcodes are valid and have no promlebs. In Stockholm area zip codes all begin with “72” or “41”, however some of zip codes were outside this region.\n",
    "In the following code, I modify the function to clean zip code, change xxx xx-xxxx format into 5 digits format, to remove the blank in the middle and create a consistant zipcode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update_zipcode(zipcode):\n",
    "    zipcode= zipcode.replace(\" \",\"\")\n",
    "    zipcodeChar = re.findall('[a-zA-Z]*', zipcode)\n",
    "    if zipcodeChar:\n",
    "        zipcodeChar = zipcodeChar[0]\n",
    "        zipcodeChar.strip()  #removes all whitespace at the start and end,including spaces,tabs,newlines and carriage returns\n",
    "        \n",
    "        return ((re.findall(r'\\d+', zipcode))[0]) \n",
    "    \n",
    "for street_type, ways in stokholm_zipcode.iteritems():\n",
    "    for zipcodez in ways:\n",
    "        update_name = update_zipcode(zipcodez)           \n",
    "#        print zipcodez, \"updated to:\", update_name        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After auditing is completed it is the time to create tables of data to be inserted into a sql database. 1)parse the data 2)transforme data from document format to tabular format 3)create csv files for each table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for this section I used some of the last udacity's quiz code directly \n",
    "OSM_PATH = \"sample.osm\"\n",
    "\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "                     \n",
    "troublemaker = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "SCHEMA = schema.schema\n",
    "\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "\n",
    "#The shape_element code is writen by myself however I was first inspired by this previous submission:https://github.com/davidventuri/udacity-dand/blob/master/p3/quizzes/preparing_for_database.py\n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "                  problem_chars=troublemaker, default_tag_type='regular'):\n",
    "\n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = [] \n",
    "\n",
    "    if element.tag == 'node':\n",
    "        for attrib, value in element.attrib.iteritems():\n",
    "            if attrib in node_attr_fields:\n",
    "                node_attribs[attrib] = value\n",
    "        \n",
    "        for secondary in element.iter():\n",
    "            if secondary.tag == 'tag':\n",
    "                if problem_chars.match(secondary.attrib['k']) is not None:\n",
    "                    continue\n",
    "                else:\n",
    "                    new = new_tagDict(element, secondary, default_tag_type)\n",
    "                    tags.append(new)\n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "    elif element.tag == 'way':\n",
    "        for attrib, value in element.attrib.iteritems():\n",
    "            if attrib in way_attr_fields:\n",
    "                way_attribs[attrib] = value\n",
    "                \n",
    "        counter = 0\n",
    "        for secondary in element.iter():\n",
    "            if secondary.tag == 'tag':\n",
    "                if problem_chars.match(secondary.attrib['k']) is not None:\n",
    "                    continue\n",
    "                else:\n",
    "                    new = new_tagDict(element, secondary, default_tag_type)\n",
    "                    tags.append(new)\n",
    "            if secondary.tag == 'nd':\n",
    "                something_new = {}\n",
    "                something_new['id'] = element.attrib['id']\n",
    "                something_new['node_id'] = secondary.attrib['ref']\n",
    "                something_new['position'] = counter\n",
    "                counter += 1\n",
    "                way_nodes.append(something_new)\n",
    "        \n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "\n",
    "\n",
    "def new_tagDict(element, secondary, default_tag_type): #Load a new tag dict to go into the list of dicts for way_tags, node_tags\n",
    "    \n",
    "    new = {}\n",
    "    new['id'] = element.attrib['id']\n",
    "    if \":\" not in secondary.attrib['k']:\n",
    "        new['key'] = secondary.attrib['k']\n",
    "        new['type'] = default_tag_type\n",
    "    else:\n",
    "        post_colon = secondary.attrib['k'].index(\":\") + 1\n",
    "        new['key'] = secondary.attrib['k'][post_colon:]\n",
    "        new['type'] = secondary.attrib['k'][:post_colon - 1]\n",
    "    new['value'] = secondary.attrib['v']\n",
    "#    print \"!23123\"\n",
    "#    print secondary.attrib['v']\n",
    "#    print\"!2312\"\n",
    "    return new\n",
    "\n",
    "\n",
    "\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')): # if it is the right type of tag then Yield the element\n",
    "    \n",
    "\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, unicode) else v) for k, v in row.items()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "                       \n",
    "def validate_element(element, validator, schema=SCHEMA): #Raise ValidationError if element does not match schema\"\"\"\n",
    "    \n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.iteritems())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_string = pprint.pformat(errors)\n",
    "\n",
    "        raise Exception(message_string.format(field, error_string))\n",
    "\n",
    "def processing_map(file_in, validate):\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'w') as nodes_file, codecs.open(NODE_TAGS_PATH, 'w') as nodes_tags_file, \\\n",
    "         codecs.open(WAYS_PATH, 'w') as ways_file, codecs.open(WAY_NODES_PATH, 'w') as way_nodes_file, \\\n",
    "         codecs.open(WAY_TAGS_PATH, 'w') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "\n",
    "        validator = cerberus.Validator()\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            element1 = shape_element(element)\n",
    "            if element1:\n",
    "                if validate is True:\n",
    "                    validate_element(element1, validator)\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(element1['node'])\n",
    "                    node_tags_writer.writerows(element1['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(element1['way'])\n",
    "                    way_nodes_writer.writerows(element1['way_nodes'])\n",
    "                    way_tags_writer.writerows(element1['way_tags'])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    result= processing_map(OSM_PATH, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlite_file = 'mydb.db' # name of the sqlite database file to be created\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect(sqlite_file)\n",
    "cur = conn.cursor()\n",
    "cur.execute('DROP TABLE IF EXISTS ways')\n",
    "conn.commit()\n",
    "cur.execute(\"CREATE TABLE ways (id,user,uid,version,changeset,timestamp);\") \n",
    "conn.commit()\n",
    "with open('ways.csv','rb') as fin:\n",
    "    delimiter = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db = [(i['id'].decode(\"utf_8\"), i['user'].decode(\"utf_8\"), i['uid'].decode(\"utf_8\"), i['version'].decode(\"utf_8\"), i['changeset'].decode(\"utf_8\"),i['timestamp'].decode(\"utf_8\")) for i in delimiter]\n",
    "\n",
    "cur.executemany(\"INSERT INTO ways (id,user,uid,version,changeset,timestamp) VALUES (?, ?,?,?, ?,?);\", to_db)\n",
    "conn.commit()\n",
    "\n",
    "cur.execute(\"SELECT * FROM ways\")\n",
    "all_rows=cur.fetchall()\n",
    "#print('1):')\n",
    "#print (all_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cur.execute('DROP TABLE IF EXISTS nodes')\n",
    "conn.commit()\n",
    "cur.execute(\"CREATE TABLE nodes (id,lat,lon,user,uid,version,changeset,timestamp);\") \n",
    "conn.commit()\n",
    "with open('nodes.csv','rb') as fin: \n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db = [(i['id'].decode(\"utf_8\"), i['lat'].decode(\"utf_8\"),i['lon'].decode(\"utf_8\"),i['user'].decode(\"utf_8\"),i['uid'].decode(\"utf_8\"),i['version'].decode(\"utf_8\"),i['changeset'].decode(\"utf_8\"),i['timestamp'].decode(\"utf_8\")) for i in dr]\n",
    "\n",
    "cur.executemany(\"INSERT INTO nodes (id,lat,lon,user,uid,version,changeset,timestamp) VALUES (?, ?,?,?,?,?,?,?);\", to_db)\n",
    "conn.commit()\n",
    "\n",
    "cur.execute(\"SELECT * FROM nodes\")\n",
    "all_rows=cur.fetchall()\n",
    "#print('1):')\n",
    "#pprint.pprint (all_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cur.execute('DROP TABLE IF EXISTS nodes_tags')\n",
    "conn.commit()\n",
    "cur.execute(\"CREATE TABLE nodes_tags (id INTEGER,key TEXT,value TEXT,type TEXT)\") \n",
    "conn.commit()\n",
    "with open('nodes_tags.csv','rb') as fin: \n",
    "    dr = csv.DictReader(fin)\n",
    "    to_db = [(i['id'].decode(\"utf_8\"), i['key'].decode(\"utf_8\"),i['value'].decode(\"utf_8\"),i['type'].decode(\"utf_8\")) for i in dr]\n",
    "\n",
    "cur.executemany(\"INSERT INTO nodes_tags (id,key,value,type) VALUES (?, ?,?,?);\", to_db)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cur.execute('DROP TABLE IF EXISTS ways_tags')\n",
    "conn.commit()\n",
    "cur.execute(\"CREATE TABLE ways_tags (id INTEGER,key TEXT,value TEXT,type TEXT)\") \n",
    "conn.commit()\n",
    "with open('ways_tags.csv','rb') as fin: # `with` statement available in 2.5+\n",
    "    # csv.DictReader uses first line in file for column headings by default\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db = [(i['id'].decode(\"utf_8\"), i['key'].decode(\"utf_8\"),i['value'].decode(\"utf_8\"),i['type'].decode(\"utf_8\")) for i in dr]\n",
    "\n",
    "cur.executemany(\"INSERT INTO ways_tags (id,key,value,type) VALUES (?, ?,?,?);\", to_db)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cur.execute('DROP TABLE IF EXISTS ways_nodes')\n",
    "conn.commit()\n",
    "cur.execute(\"CREATE TABLE ways_nodes (id , node_id, position)\") \n",
    "conn.commit()\n",
    "with open('ways_nodes.csv','rb') as fin: # `__with` statement available in 2.5+\n",
    "    # csv.DictReader uses first line in file for column headings by default\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db = [(i['id'].decode(\"utf_8\"), i['node_id'].decode(\"utf_8\"),i['position'].decode(\"utf_8\")) for i in dr]\n",
    "\n",
    "cur.executemany(\"INSERT INTO ways_nodes (id,node_id,position) VALUES (?, ?,?);\", to_db)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1):\n",
      "[(311955, u'ref', u'143', u'regular'),\n",
      " (311955, u'name', u'Trafikplats Saltskog', u'regular'),\n",
      " (311955, u'highway', u'motorway_junction', u'regular'),\n",
      " (311955, u'road_ref', u'E 20', u'exit')]\n"
     ]
    }
   ],
   "source": [
    "cur.execute(\"SELECT * FROM nodes_tags WHERE id IN (SELECT DISTINCT(id) FROM nodes_tags WHERE key='road_ref' AND value='E 20')\")\n",
    "road_ref=cur.fetchall()\n",
    "print('1):')\n",
    "pprint.pprint (road_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional ideas:\n",
    "List of top 20 Amenities in Stockholm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 top amenities in stockholm:\n",
      "[(u'bench', 227),\n",
      " (u'restaurant', 209),\n",
      " (u'fast_food', 114),\n",
      " (u'cafe', 92),\n",
      " (u'post_box', 92),\n",
      " (u'parking', 87),\n",
      " (u'recycling', 71),\n",
      " (u'waste_basket', 67),\n",
      " (u'shelter', 60),\n",
      " (u'bicycle_parking', 50),\n",
      " (u'toilets', 37),\n",
      " (u'fuel', 36),\n",
      " (u'pharmacy', 28),\n",
      " (u'bank', 26),\n",
      " (u'school', 25),\n",
      " (u'atm', 22),\n",
      " (u'pub', 22),\n",
      " (u'ferry_terminal', 21),\n",
      " (u'kindergarten', 20),\n",
      " (u'bbq', 14)]\n"
     ]
    }
   ],
   "source": [
    "cur.execute(\"SELECT value, COUNT(*) as num \\\n",
    "            FROM nodes_tags \\\n",
    "           WHERE key='amenity' \\\n",
    "           GROUP BY value \\\n",
    "           ORDER BY num DESC \\\n",
    "           LIMIT 20;\")\n",
    "amenity= cur.fetchall()\n",
    "print('20 top amenities in stockholm:')\n",
    "pprint.pprint (amenity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview of the data\n",
    "This section contains basic statistics about the dataset, the SQL queries used to gather them, and some additional ideas about the data in context.\n",
    "File Size:\n",
    "\n",
    "Stockholm.osm: 1.29 GB\n",
    "nodes_csv: 49.5 MB\n",
    "nodes_tags.csv: 163.4 KB\n",
    "ways_csv: 4.1 MB\n",
    "ways_nodes.csv: 17.9 MB\n",
    "ways_tags.csv: 5.3 MB\n",
    "\n",
    "Number of Nodes\n",
    "Number of Ways\n",
    "Number of unique users\n",
    "Top 5 contrinuters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of nodes:\n",
      "610544\n"
     ]
    }
   ],
   "source": [
    "cur.execute(\"SELECT COUNT(*) FROM nodes\")\n",
    "nodes= cur.fetchall()\n",
    "print('number of nodes:')\n",
    "print nodes[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of ways:\n",
      "69782\n"
     ]
    }
   ],
   "source": [
    "cur.execute(\"SELECT COUNT(*) FROM ways\")\n",
    "ways= cur.fetchall()\n",
    "print('number of ways:')\n",
    "print ways[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique users:\n",
      "1880\n"
     ]
    }
   ],
   "source": [
    "cur.execute(\"SELECT COUNT(DISTINCT(e.uid)) FROM (SELECT uid FROM nodes UNION SELECT uid FROM ways) e\")\n",
    "unique= cur.fetchall()\n",
    "print( 'unique users:')\n",
    "print unique[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 top contributors:\n",
      "[(u'MichaelCollinson', 68249), (u'Fringillus', 65250), (u'emj', 42343), (u'huven', 39093), (u'jordgubbe', 26045)]\n"
     ]
    }
   ],
   "source": [
    "cur.execute(\"SELECT e.user, COUNT(*) as num FROM (SELECT user FROM nodes UNION ALL SELECT user FROM ways) e GROUP BY e.user ORDER BY num DESC LIMIT 5;\")\n",
    "unique= cur.fetchall()\n",
    "print( '5 top contributors:')\n",
    "print unique\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "From the process of auditing, it is notable that the dataset is fairly well-cleaned even though there is some minor error such as inconsistent postal codes. Since there are thousands of contributing human users, so it is inevitable to have much human input error. In addition, OpenStreetMaps is an open source project hence there’re still a lot of areas either missed or outdated. This is applicable also to my favorite city Stockholm. So I hope OpenStreetMaps can obtain these data from other open data sources. As an example, I am very inspired by a very cool startup based in Malmö/Sweden called 'Mapillary'(a service for sharing geotagged photos).\n",
    "I live in the south of Sweden, meaning that biking and walking are part of our daily life. It would be very interesting to have tags for sidewalks or bikeways included in the data. To emphasis on bike paths and walk paths for the convenience of end user, OpenStreetMap could map them properly. Its worth nothing that sometimes walk side are mistagged, as a park or other green areas. Hence the solution is new proper tags and correcting the already existing but mistagged tags."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
